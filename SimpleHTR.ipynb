{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ethan830/CompVision-AI-ML/blob/main/SimpleHTR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdwRqjceXQsn",
        "outputId": "5d734941-24ac-4dbd-9e34-f2a27dfe8c12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SimpleHTR'...\n",
            "remote: Enumerating objects: 548, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 548 (delta 0), reused 1 (delta 0), pack-reused 544\u001b[K\n",
            "Receiving objects: 100% (548/548), 78.95 MiB | 44.13 MiB/s, done.\n",
            "Resolving deltas: 100% (300/300), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting path\n",
            "  Downloading path-16.5.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: path\n",
            "Successfully installed path-16.5.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.6.0.66)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/githubharald/SimpleHTR\n",
        "!pip install path\n",
        "!pip install opencv-python\n",
        "\n",
        "#https://github.com/githubharald/SimpleHTR/tree/97c2512f593760b14669b37a159ead2f1e54961b"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.argv\n",
        "\n",
        "import argparse\n",
        "import pickle\n",
        "\n",
        "import cv2\n",
        "import lmdb\n",
        "from path import Path\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--data_dir', type=Path, required=True)\n",
        "\n",
        "args = parser.parse_known_args()\n",
        "#args = parser.parse_args() #original line\n",
        "\n",
        "# 2GB is enough for IAM dataset\n",
        "assert not (args.data_dir / 'lmdb').exists()\n",
        "env = lmdb.open(str(args.data_dir / 'lmdb'), map_size=1024 * 1024 * 1024 * 2)\n",
        "\n",
        "# go over all png files\n",
        "fn_imgs = list((args.data_dir / 'img').walkfiles('*.png'))\n",
        "\n",
        "# and put the imgs into lmdb as pickled grayscale imgs\n",
        "with env.begin(write=True) as txn:\n",
        "    for i, fn_img in enumerate(fn_imgs):\n",
        "        print(i, len(fn_imgs))\n",
        "        img = cv2.imread(fn_img, cv2.IMREAD_GRAYSCALE)\n",
        "        basename = fn_img.basename()\n",
        "        txn.put(basename.encode(\"ascii\"), pickle.dumps(img))\n",
        "\n",
        "env.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "id": "FcdK3dpxXgYU",
        "outputId": "b2541548-0cfd-4e71-dba9-3a5944147678"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: ipykernel_launcher.py [-h] --data_dir DATA_DIR\n",
            "ipykernel_launcher.py: error: the following arguments are required: --data_dir\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import random\n",
        "from collections import namedtuple\n",
        "from typing import Tuple\n",
        "\n",
        "import cv2\n",
        "import lmdb\n",
        "import numpy as np\n",
        "from path import Path\n",
        "\n",
        "Sample = namedtuple('Sample', 'gt_text, file_path')\n",
        "Batch = namedtuple('Batch', 'imgs, gt_texts, batch_size')\n",
        "\n",
        "\n",
        "class DataLoaderIAM:\n",
        "    \"\"\"\n",
        "    Loads data which corresponds to IAM format,\n",
        "    see: http://www.fki.inf.unibe.ch/databases/iam-handwriting-database\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 data_dir: Path,\n",
        "                 batch_size: int,\n",
        "                 data_split: float = 0.95,\n",
        "                 fast: bool = True) -> None:\n",
        "        \"\"\"Loader for dataset.\"\"\"\n",
        "\n",
        "        assert data_dir.exists()\n",
        "\n",
        "        self.fast = fast\n",
        "        if fast:\n",
        "            self.env = lmdb.open(str(data_dir / 'lmdb'), readonly=True)\n",
        "\n",
        "        self.data_augmentation = False\n",
        "        self.curr_idx = 0\n",
        "        self.batch_size = batch_size\n",
        "        self.samples = []\n",
        "\n",
        "        f = open(data_dir / 'gt/words.txt')\n",
        "        chars = set()\n",
        "        bad_samples_reference = ['a01-117-05-02', 'r06-022-03-05']  # known broken images in IAM dataset\n",
        "        for line in f:\n",
        "            # ignore empty and comment lines\n",
        "            line = line.strip()\n",
        "            if not line or line[0] == '#':\n",
        "                continue\n",
        "\n",
        "            line_split = line.split(' ')\n",
        "            assert len(line_split) >= 9\n",
        "\n",
        "            # filename: part1-part2-part3 --> part1/part1-part2/part1-part2-part3.png\n",
        "            file_name_split = line_split[0].split('-')\n",
        "            file_name_subdir1 = file_name_split[0]\n",
        "            file_name_subdir2 = f'{file_name_split[0]}-{file_name_split[1]}'\n",
        "            file_base_name = line_split[0] + '.png'\n",
        "            file_name = data_dir / 'img' / file_name_subdir1 / file_name_subdir2 / file_base_name\n",
        "\n",
        "            if line_split[0] in bad_samples_reference:\n",
        "                print('Ignoring known broken image:', file_name)\n",
        "                continue\n",
        "\n",
        "            # GT text are columns starting at 9\n",
        "            gt_text = ' '.join(line_split[8:])\n",
        "            chars = chars.union(set(list(gt_text)))\n",
        "\n",
        "            # put sample into list\n",
        "            self.samples.append(Sample(gt_text, file_name))\n",
        "\n",
        "        # split into training and validation set: 95% - 5%\n",
        "        split_idx = int(data_split * len(self.samples))\n",
        "        self.train_samples = self.samples[:split_idx]\n",
        "        self.validation_samples = self.samples[split_idx:]\n",
        "\n",
        "        # put words into lists\n",
        "        self.train_words = [x.gt_text for x in self.train_samples]\n",
        "        self.validation_words = [x.gt_text for x in self.validation_samples]\n",
        "\n",
        "        # start with train set\n",
        "        self.train_set()\n",
        "\n",
        "        # list of all chars in dataset\n",
        "        self.char_list = sorted(list(chars))\n",
        "\n",
        "    def train_set(self) -> None:\n",
        "        \"\"\"Switch to randomly chosen subset of training set.\"\"\"\n",
        "        self.data_augmentation = True\n",
        "        self.curr_idx = 0\n",
        "        random.shuffle(self.train_samples)\n",
        "        self.samples = self.train_samples\n",
        "        self.curr_set = 'train'\n",
        "\n",
        "    def validation_set(self) -> None:\n",
        "        \"\"\"Switch to validation set.\"\"\"\n",
        "        self.data_augmentation = False\n",
        "        self.curr_idx = 0\n",
        "        self.samples = self.validation_samples\n",
        "        self.curr_set = 'val'\n",
        "\n",
        "    def get_iterator_info(self) -> Tuple[int, int]:\n",
        "        \"\"\"Current batch index and overall number of batches.\"\"\"\n",
        "        if self.curr_set == 'train':\n",
        "            num_batches = int(np.floor(len(self.samples) / self.batch_size))  # train set: only full-sized batches\n",
        "        else:\n",
        "            num_batches = int(np.ceil(len(self.samples) / self.batch_size))  # val set: allow last batch to be smaller\n",
        "        curr_batch = self.curr_idx // self.batch_size + 1\n",
        "        return curr_batch, num_batches\n",
        "\n",
        "    def has_next(self) -> bool:\n",
        "        \"\"\"Is there a next element?\"\"\"\n",
        "        if self.curr_set == 'train':\n",
        "            return self.curr_idx + self.batch_size <= len(self.samples)  # train set: only full-sized batches\n",
        "        else:\n",
        "            return self.curr_idx < len(self.samples)  # val set: allow last batch to be smaller\n",
        "\n",
        "    def _get_img(self, i: int) -> np.ndarray:\n",
        "        if self.fast:\n",
        "            with self.env.begin() as txn:\n",
        "                basename = Path(self.samples[i].file_path).basename()\n",
        "                data = txn.get(basename.encode(\"ascii\"))\n",
        "                img = pickle.loads(data)\n",
        "        else:\n",
        "            img = cv2.imread(self.samples[i].file_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        return img\n",
        "\n",
        "    def get_next(self) -> Batch:\n",
        "        \"\"\"Get next element.\"\"\"\n",
        "        batch_range = range(self.curr_idx, min(self.curr_idx + self.batch_size, len(self.samples)))\n",
        "\n",
        "        imgs = [self._get_img(i) for i in batch_range]\n",
        "        gt_texts = [self.samples[i].gt_text for i in batch_range]\n",
        "\n",
        "        self.curr_idx += self.batch_size\n",
        "        return Batch(imgs, gt_texts, len(imgs))\n"
      ],
      "metadata": {
        "id": "NO7V2i6_X61Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import json\n",
        "from typing import Tuple, List\n",
        "\n",
        "import cv2\n",
        "import editdistance\n",
        "from path import Path\n",
        "\n",
        "from dataloader_iam import DataLoaderIAM, Batch\n",
        "from model import Model, DecoderType\n",
        "from preprocessor import Preprocessor\n",
        "\n",
        "\n",
        "class FilePaths:\n",
        "    \"\"\"Filenames and paths to data.\"\"\"\n",
        "    fn_char_list = '../model/charList.txt'\n",
        "    fn_summary = '../model/summary.json'\n",
        "    fn_corpus = '../data/corpus.txt'\n",
        "\n",
        "\n",
        "def get_img_height() -> int:\n",
        "    \"\"\"Fixed height for NN.\"\"\"\n",
        "    return 32\n",
        "\n",
        "\n",
        "def get_img_size(line_mode: bool = False) -> Tuple[int, int]:\n",
        "    \"\"\"Height is fixed for NN, width is set according to training mode (single words or text lines).\"\"\"\n",
        "    if line_mode:\n",
        "        return 256, get_img_height()\n",
        "    return 128, get_img_height()\n",
        "\n",
        "\n",
        "def write_summary(char_error_rates: List[float], word_accuracies: List[float]) -> None:\n",
        "    \"\"\"Writes training summary file for NN.\"\"\"\n",
        "    with open(FilePaths.fn_summary, 'w') as f:\n",
        "        json.dump({'charErrorRates': char_error_rates, 'wordAccuracies': word_accuracies}, f)\n",
        "\n",
        "\n",
        "def char_list_from_file() -> List[str]:\n",
        "    with open(FilePaths.fn_char_list) as f:\n",
        "        return list(f.read())\n",
        "\n",
        "\n",
        "def train(model: Model,\n",
        "          loader: DataLoaderIAM,\n",
        "          line_mode: bool,\n",
        "          early_stopping: int = 25) -> None:\n",
        "    \"\"\"Trains NN.\"\"\"\n",
        "    epoch = 0  # number of training epochs since start\n",
        "    summary_char_error_rates = []\n",
        "    summary_word_accuracies = []\n",
        "    preprocessor = Preprocessor(get_img_size(line_mode), data_augmentation=True, line_mode=line_mode)\n",
        "    best_char_error_rate = float('inf')  # best validation character error rate\n",
        "    no_improvement_since = 0  # number of epochs no improvement of character error rate occurred\n",
        "    # stop training after this number of epochs without improvement\n",
        "    while True:\n",
        "        epoch += 1\n",
        "        print('Epoch:', epoch)\n",
        "\n",
        "        # train\n",
        "        print('Train NN')\n",
        "        loader.train_set()\n",
        "        while loader.has_next():\n",
        "            iter_info = loader.get_iterator_info()\n",
        "            batch = loader.get_next()\n",
        "            batch = preprocessor.process_batch(batch)\n",
        "            loss = model.train_batch(batch)\n",
        "            print(f'Epoch: {epoch} Batch: {iter_info[0]}/{iter_info[1]} Loss: {loss}')\n",
        "\n",
        "        # validate\n",
        "        char_error_rate, word_accuracy = validate(model, loader, line_mode)\n",
        "\n",
        "        # write summary\n",
        "        summary_char_error_rates.append(char_error_rate)\n",
        "        summary_word_accuracies.append(word_accuracy)\n",
        "        write_summary(summary_char_error_rates, summary_word_accuracies)\n",
        "\n",
        "        # if best validation accuracy so far, save model parameters\n",
        "        if char_error_rate < best_char_error_rate:\n",
        "            print('Character error rate improved, save model')\n",
        "            best_char_error_rate = char_error_rate\n",
        "            no_improvement_since = 0\n",
        "            model.save()\n",
        "        else:\n",
        "            print(f'Character error rate not improved, best so far: {char_error_rate * 100.0}%')\n",
        "            no_improvement_since += 1\n",
        "\n",
        "        # stop training if no more improvement in the last x epochs\n",
        "        if no_improvement_since >= early_stopping:\n",
        "            print(f'No more improvement since {early_stopping} epochs. Training stopped.')\n",
        "            break\n",
        "\n",
        "\n",
        "def validate(model: Model, loader: DataLoaderIAM, line_mode: bool) -> Tuple[float, float]:\n",
        "    \"\"\"Validates NN.\"\"\"\n",
        "    print('Validate NN')\n",
        "    loader.validation_set()\n",
        "    preprocessor = Preprocessor(get_img_size(line_mode), line_mode=line_mode)\n",
        "    num_char_err = 0\n",
        "    num_char_total = 0\n",
        "    num_word_ok = 0\n",
        "    num_word_total = 0\n",
        "    while loader.has_next():\n",
        "        iter_info = loader.get_iterator_info()\n",
        "        print(f'Batch: {iter_info[0]} / {iter_info[1]}')\n",
        "        batch = loader.get_next()\n",
        "        batch = preprocessor.process_batch(batch)\n",
        "        recognized, _ = model.infer_batch(batch)\n",
        "\n",
        "        print('Ground truth -> Recognized')\n",
        "        for i in range(len(recognized)):\n",
        "            num_word_ok += 1 if batch.gt_texts[i] == recognized[i] else 0\n",
        "            num_word_total += 1\n",
        "            dist = editdistance.eval(recognized[i], batch.gt_texts[i])\n",
        "            num_char_err += dist\n",
        "            num_char_total += len(batch.gt_texts[i])\n",
        "            print('[OK]' if dist == 0 else '[ERR:%d]' % dist, '\"' + batch.gt_texts[i] + '\"', '->',\n",
        "                  '\"' + recognized[i] + '\"')\n",
        "\n",
        "    # print validation result\n",
        "    char_error_rate = num_char_err / num_char_total\n",
        "    word_accuracy = num_word_ok / num_word_total\n",
        "    print(f'Character error rate: {char_error_rate * 100.0}%. Word accuracy: {word_accuracy * 100.0}%.')\n",
        "    return char_error_rate, word_accuracy\n",
        "\n",
        "\n",
        "def infer(model: Model, fn_img: Path) -> None:\n",
        "    \"\"\"Recognizes text in image provided by file path.\"\"\"\n",
        "    img = cv2.imread(fn_img, cv2.IMREAD_GRAYSCALE)\n",
        "    assert img is not None\n",
        "\n",
        "    preprocessor = Preprocessor(get_img_size(), dynamic_width=True, padding=16)\n",
        "    img = preprocessor.process_img(img)\n",
        "\n",
        "    batch = Batch([img], None, 1)\n",
        "    recognized, probability = model.infer_batch(batch, True)\n",
        "    print(f'Recognized: \"{recognized[0]}\"')\n",
        "    print(f'Probability: {probability[0]}')\n",
        "\n",
        "\n",
        "def parse_args() -> argparse.Namespace:\n",
        "    \"\"\"Parses arguments from the command line.\"\"\"\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument('--mode', choices=['train', 'validate', 'infer'], default='infer')\n",
        "    parser.add_argument('--decoder', choices=['bestpath', 'beamsearch', 'wordbeamsearch'], default='bestpath')\n",
        "    parser.add_argument('--batch_size', help='Batch size.', type=int, default=100)\n",
        "    parser.add_argument('--data_dir', help='Directory containing IAM dataset.', type=Path, required=False)\n",
        "    parser.add_argument('--fast', help='Load samples from LMDB.', action='store_true')\n",
        "    parser.add_argument('--line_mode', help='Train to read text lines instead of single words.', action='store_true')\n",
        "    parser.add_argument('--img_file', help='Image used for inference.', type=Path, default='../data/word.png')\n",
        "    parser.add_argument('--early_stopping', help='Early stopping epochs.', type=int, default=25)\n",
        "    parser.add_argument('--dump', help='Dump output of NN to CSV file(s).', action='store_true')\n",
        "\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function.\"\"\"\n",
        "\n",
        "    # parse arguments and set CTC decoder\n",
        "    args = parse_args()\n",
        "    decoder_mapping = {'bestpath': DecoderType.BestPath,\n",
        "                       'beamsearch': DecoderType.BeamSearch,\n",
        "                       'wordbeamsearch': DecoderType.WordBeamSearch}\n",
        "    decoder_type = decoder_mapping[args.decoder]\n",
        "\n",
        "    # train the model\n",
        "    if args.mode == 'train':\n",
        "        loader = DataLoaderIAM(args.data_dir, args.batch_size, fast=args.fast)\n",
        "\n",
        "        # when in line mode, take care to have a whitespace in the char list\n",
        "        char_list = loader.char_list\n",
        "        if args.line_mode and ' ' not in char_list:\n",
        "            char_list = [' '] + char_list\n",
        "\n",
        "        # save characters and words\n",
        "        with open(FilePaths.fn_char_list, 'w') as f:\n",
        "            f.write(''.join(char_list))\n",
        "\n",
        "        with open(FilePaths.fn_corpus, 'w') as f:\n",
        "            f.write(' '.join(loader.train_words + loader.validation_words))\n",
        "\n",
        "        model = Model(char_list, decoder_type)\n",
        "        train(model, loader, line_mode=args.line_mode, early_stopping=args.early_stopping)\n",
        "\n",
        "    # evaluate it on the validation set\n",
        "    elif args.mode == 'validate':\n",
        "        loader = DataLoaderIAM(args.data_dir, args.batch_size, fast=args.fast)\n",
        "        model = Model(char_list_from_file(), decoder_type, must_restore=True)\n",
        "        validate(model, loader, args.line_mode)\n",
        "\n",
        "    # infer text on test image\n",
        "    elif args.mode == 'infer':\n",
        "        model = Model(char_list_from_file(), decoder_type, must_restore=True, dump=args.dump)\n",
        "        infer(model, args.img_file)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "fRw1YHbKX8pI",
        "outputId": "c725f08f-05c1-4ded-ecdc-63c288048894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-88d9f68ecc11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdataloader_iam\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoaderIAM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDecoderType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreprocessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dataloader_iam'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "from typing import List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from dataloader_iam import Batch\n",
        "\n",
        "# Disable eager mode\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "\n",
        "class DecoderType:\n",
        "    \"\"\"CTC decoder types.\"\"\"\n",
        "    BestPath = 0\n",
        "    BeamSearch = 1\n",
        "    WordBeamSearch = 2\n",
        "\n",
        "\n",
        "class Model:\n",
        "    \"\"\"Minimalistic TF model for HTR.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 char_list: List[str],\n",
        "                 decoder_type: str = DecoderType.BestPath,\n",
        "                 must_restore: bool = False,\n",
        "                 dump: bool = False) -> None:\n",
        "        \"\"\"Init model: add CNN, RNN and CTC and initialize TF.\"\"\"\n",
        "        self.dump = dump\n",
        "        self.char_list = char_list\n",
        "        self.decoder_type = decoder_type\n",
        "        self.must_restore = must_restore\n",
        "        self.snap_ID = 0\n",
        "\n",
        "        # Whether to use normalization over a batch or a population\n",
        "        self.is_train = tf.compat.v1.placeholder(tf.bool, name='is_train')\n",
        "\n",
        "        # input image batch\n",
        "        self.input_imgs = tf.compat.v1.placeholder(tf.float32, shape=(None, None, None))\n",
        "\n",
        "        # setup CNN, RNN and CTC\n",
        "        self.setup_cnn()\n",
        "        self.setup_rnn()\n",
        "        self.setup_ctc()\n",
        "\n",
        "        # setup optimizer to train NN\n",
        "        self.batches_trained = 0\n",
        "        self.update_ops = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)\n",
        "        with tf.control_dependencies(self.update_ops):\n",
        "            self.optimizer = tf.compat.v1.train.AdamOptimizer().minimize(self.loss)\n",
        "\n",
        "        # initialize TF\n",
        "        self.sess, self.saver = self.setup_tf()\n",
        "\n",
        "    def setup_cnn(self) -> None:\n",
        "        \"\"\"Create CNN layers.\"\"\"\n",
        "        cnn_in4d = tf.expand_dims(input=self.input_imgs, axis=3)\n",
        "\n",
        "        # list of parameters for the layers\n",
        "        kernel_vals = [5, 5, 3, 3, 3]\n",
        "        feature_vals = [1, 32, 64, 128, 128, 256]\n",
        "        stride_vals = pool_vals = [(2, 2), (2, 2), (1, 2), (1, 2), (1, 2)]\n",
        "        num_layers = len(stride_vals)\n",
        "\n",
        "        # create layers\n",
        "        pool = cnn_in4d  # input to first CNN layer\n",
        "        for i in range(num_layers):\n",
        "            kernel = tf.Variable(\n",
        "                tf.random.truncated_normal([kernel_vals[i], kernel_vals[i], feature_vals[i], feature_vals[i + 1]],\n",
        "                                           stddev=0.1))\n",
        "            conv = tf.nn.conv2d(input=pool, filters=kernel, padding='SAME', strides=(1, 1, 1, 1))\n",
        "            conv_norm = tf.compat.v1.layers.batch_normalization(conv, training=self.is_train)\n",
        "            relu = tf.nn.relu(conv_norm)\n",
        "            pool = tf.nn.max_pool2d(input=relu, ksize=(1, pool_vals[i][0], pool_vals[i][1], 1),\n",
        "                                    strides=(1, stride_vals[i][0], stride_vals[i][1], 1), padding='VALID')\n",
        "\n",
        "        self.cnn_out_4d = pool\n",
        "\n",
        "    def setup_rnn(self) -> None:\n",
        "        \"\"\"Create RNN layers.\"\"\"\n",
        "        rnn_in3d = tf.squeeze(self.cnn_out_4d, axis=[2])\n",
        "\n",
        "        # basic cells which is used to build RNN\n",
        "        num_hidden = 256\n",
        "        cells = [tf.compat.v1.nn.rnn_cell.LSTMCell(num_units=num_hidden, state_is_tuple=True) for _ in\n",
        "                 range(2)]  # 2 layers\n",
        "\n",
        "        # stack basic cells\n",
        "        stacked = tf.compat.v1.nn.rnn_cell.MultiRNNCell(cells, state_is_tuple=True)\n",
        "\n",
        "        # bidirectional RNN\n",
        "        # BxTxF -> BxTx2H\n",
        "        (fw, bw), _ = tf.compat.v1.nn.bidirectional_dynamic_rnn(cell_fw=stacked, cell_bw=stacked, inputs=rnn_in3d,\n",
        "                                                                dtype=rnn_in3d.dtype)\n",
        "\n",
        "        # BxTxH + BxTxH -> BxTx2H -> BxTx1X2H\n",
        "        concat = tf.expand_dims(tf.concat([fw, bw], 2), 2)\n",
        "\n",
        "        # project output to chars (including blank): BxTx1x2H -> BxTx1xC -> BxTxC\n",
        "        kernel = tf.Variable(tf.random.truncated_normal([1, 1, num_hidden * 2, len(self.char_list) + 1], stddev=0.1))\n",
        "        self.rnn_out_3d = tf.squeeze(tf.nn.atrous_conv2d(value=concat, filters=kernel, rate=1, padding='SAME'),\n",
        "                                     axis=[2])\n",
        "\n",
        "    def setup_ctc(self) -> None:\n",
        "        \"\"\"Create CTC loss and decoder.\"\"\"\n",
        "        # BxTxC -> TxBxC\n",
        "        self.ctc_in_3d_tbc = tf.transpose(a=self.rnn_out_3d, perm=[1, 0, 2])\n",
        "        # ground truth text as sparse tensor\n",
        "        self.gt_texts = tf.SparseTensor(tf.compat.v1.placeholder(tf.int64, shape=[None, 2]),\n",
        "                                        tf.compat.v1.placeholder(tf.int32, [None]),\n",
        "                                        tf.compat.v1.placeholder(tf.int64, [2]))\n",
        "\n",
        "        # calc loss for batch\n",
        "        self.seq_len = tf.compat.v1.placeholder(tf.int32, [None])\n",
        "        self.loss = tf.reduce_mean(\n",
        "            input_tensor=tf.compat.v1.nn.ctc_loss(labels=self.gt_texts, inputs=self.ctc_in_3d_tbc,\n",
        "                                                  sequence_length=self.seq_len,\n",
        "                                                  ctc_merge_repeated=True))\n",
        "\n",
        "        # calc loss for each element to compute label probability\n",
        "        self.saved_ctc_input = tf.compat.v1.placeholder(tf.float32,\n",
        "                                                        shape=[None, None, len(self.char_list) + 1])\n",
        "        self.loss_per_element = tf.compat.v1.nn.ctc_loss(labels=self.gt_texts, inputs=self.saved_ctc_input,\n",
        "                                                         sequence_length=self.seq_len, ctc_merge_repeated=True)\n",
        "\n",
        "        # best path decoding or beam search decoding\n",
        "        if self.decoder_type == DecoderType.BestPath:\n",
        "            self.decoder = tf.nn.ctc_greedy_decoder(inputs=self.ctc_in_3d_tbc, sequence_length=self.seq_len)\n",
        "        elif self.decoder_type == DecoderType.BeamSearch:\n",
        "            self.decoder = tf.nn.ctc_beam_search_decoder(inputs=self.ctc_in_3d_tbc, sequence_length=self.seq_len,\n",
        "                                                         beam_width=50)\n",
        "        # word beam search decoding (see https://github.com/githubharald/CTCWordBeamSearch)\n",
        "        elif self.decoder_type == DecoderType.WordBeamSearch:\n",
        "            # prepare information about language (dictionary, characters in dataset, characters forming words)\n",
        "            chars = ''.join(self.char_list)\n",
        "            word_chars = open('../model/wordCharList.txt').read().splitlines()[0]\n",
        "            corpus = open('../data/corpus.txt').read()\n",
        "\n",
        "            # decode using the \"Words\" mode of word beam search\n",
        "            from word_beam_search import WordBeamSearch\n",
        "            self.decoder = WordBeamSearch(50, 'Words', 0.0, corpus.encode('utf8'), chars.encode('utf8'),\n",
        "                                          word_chars.encode('utf8'))\n",
        "\n",
        "            # the input to the decoder must have softmax already applied\n",
        "            self.wbs_input = tf.nn.softmax(self.ctc_in_3d_tbc, axis=2)\n",
        "\n",
        "    def setup_tf(self) -> Tuple[tf.compat.v1.Session, tf.compat.v1.train.Saver]:\n",
        "        \"\"\"Initialize TF.\"\"\"\n",
        "        print('Python: ' + sys.version)\n",
        "        print('Tensorflow: ' + tf.__version__)\n",
        "\n",
        "        sess = tf.compat.v1.Session()  # TF session\n",
        "\n",
        "        saver = tf.compat.v1.train.Saver(max_to_keep=1)  # saver saves model to file\n",
        "        model_dir = '../model/'\n",
        "        latest_snapshot = tf.train.latest_checkpoint(model_dir)  # is there a saved model?\n",
        "\n",
        "        # if model must be restored (for inference), there must be a snapshot\n",
        "        if self.must_restore and not latest_snapshot:\n",
        "            raise Exception('No saved model found in: ' + model_dir)\n",
        "\n",
        "        # load saved model if available\n",
        "        if latest_snapshot:\n",
        "            print('Init with stored values from ' + latest_snapshot)\n",
        "            saver.restore(sess, latest_snapshot)\n",
        "        else:\n",
        "            print('Init with new values')\n",
        "            sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "        return sess, saver\n",
        "\n",
        "    def to_sparse(self, texts: List[str]) -> Tuple[List[List[int]], List[int], List[int]]:\n",
        "        \"\"\"Put ground truth texts into sparse tensor for ctc_loss.\"\"\"\n",
        "        indices = []\n",
        "        values = []\n",
        "        shape = [len(texts), 0]  # last entry must be max(labelList[i])\n",
        "\n",
        "        # go over all texts\n",
        "        for batchElement, text in enumerate(texts):\n",
        "            # convert to string of label (i.e. class-ids)\n",
        "            label_str = [self.char_list.index(c) for c in text]\n",
        "            # sparse tensor must have size of max. label-string\n",
        "            if len(label_str) > shape[1]:\n",
        "                shape[1] = len(label_str)\n",
        "            # put each label into sparse tensor\n",
        "            for i, label in enumerate(label_str):\n",
        "                indices.append([batchElement, i])\n",
        "                values.append(label)\n",
        "\n",
        "        return indices, values, shape\n",
        "\n",
        "    def decoder_output_to_text(self, ctc_output: tuple, batch_size: int) -> List[str]:\n",
        "        \"\"\"Extract texts from output of CTC decoder.\"\"\"\n",
        "\n",
        "        # word beam search: already contains label strings\n",
        "        if self.decoder_type == DecoderType.WordBeamSearch:\n",
        "            label_strs = ctc_output\n",
        "\n",
        "        # TF decoders: label strings are contained in sparse tensor\n",
        "        else:\n",
        "            # ctc returns tuple, first element is SparseTensor\n",
        "            decoded = ctc_output[0][0]\n",
        "\n",
        "            # contains string of labels for each batch element\n",
        "            label_strs = [[] for _ in range(batch_size)]\n",
        "\n",
        "            # go over all indices and save mapping: batch -> values\n",
        "            for (idx, idx2d) in enumerate(decoded.indices):\n",
        "                label = decoded.values[idx]\n",
        "                batch_element = idx2d[0]  # index according to [b,t]\n",
        "                label_strs[batch_element].append(label)\n",
        "\n",
        "        # map labels to chars for all batch elements\n",
        "        return [''.join([self.char_list[c] for c in labelStr]) for labelStr in label_strs]\n",
        "\n",
        "    def train_batch(self, batch: Batch) -> float:\n",
        "        \"\"\"Feed a batch into the NN to train it.\"\"\"\n",
        "        num_batch_elements = len(batch.imgs)\n",
        "        max_text_len = batch.imgs[0].shape[0] // 4\n",
        "        sparse = self.to_sparse(batch.gt_texts)\n",
        "        eval_list = [self.optimizer, self.loss]\n",
        "        feed_dict = {self.input_imgs: batch.imgs, self.gt_texts: sparse,\n",
        "                     self.seq_len: [max_text_len] * num_batch_elements, self.is_train: True}\n",
        "        _, loss_val = self.sess.run(eval_list, feed_dict)\n",
        "        self.batches_trained += 1\n",
        "        return loss_val\n",
        "\n",
        "    @staticmethod\n",
        "    def dump_nn_output(rnn_output: np.ndarray) -> None:\n",
        "        \"\"\"Dump the output of the NN to CSV file(s).\"\"\"\n",
        "        dump_dir = '../dump/'\n",
        "        if not os.path.isdir(dump_dir):\n",
        "            os.mkdir(dump_dir)\n",
        "\n",
        "        # iterate over all batch elements and create a CSV file for each one\n",
        "        max_t, max_b, max_c = rnn_output.shape\n",
        "        for b in range(max_b):\n",
        "            csv = ''\n",
        "            for t in range(max_t):\n",
        "                for c in range(max_c):\n",
        "                    csv += str(rnn_output[t, b, c]) + ';'\n",
        "                csv += '\\n'\n",
        "            fn = dump_dir + 'rnnOutput_' + str(b) + '.csv'\n",
        "            print('Write dump of NN to file: ' + fn)\n",
        "            with open(fn, 'w') as f:\n",
        "                f.write(csv)\n",
        "\n",
        "    def infer_batch(self, batch: Batch, calc_probability: bool = False, probability_of_gt: bool = False):\n",
        "        \"\"\"Feed a batch into the NN to recognize the texts.\"\"\"\n",
        "\n",
        "        # decode, optionally save RNN output\n",
        "        num_batch_elements = len(batch.imgs)\n",
        "\n",
        "        # put tensors to be evaluated into list\n",
        "        eval_list = []\n",
        "\n",
        "        if self.decoder_type == DecoderType.WordBeamSearch:\n",
        "            eval_list.append(self.wbs_input)\n",
        "        else:\n",
        "            eval_list.append(self.decoder)\n",
        "\n",
        "        if self.dump or calc_probability:\n",
        "            eval_list.append(self.ctc_in_3d_tbc)\n",
        "\n",
        "        # sequence length depends on input image size (model downsizes width by 4)\n",
        "        max_text_len = batch.imgs[0].shape[0] // 4\n",
        "\n",
        "        # dict containing all tensor fed into the model\n",
        "        feed_dict = {self.input_imgs: batch.imgs, self.seq_len: [max_text_len] * num_batch_elements,\n",
        "                     self.is_train: False}\n",
        "\n",
        "        # evaluate model\n",
        "        eval_res = self.sess.run(eval_list, feed_dict)\n",
        "\n",
        "        # TF decoders: decoding already done in TF graph\n",
        "        if self.decoder_type != DecoderType.WordBeamSearch:\n",
        "            decoded = eval_res[0]\n",
        "        # word beam search decoder: decoding is done in C++ function compute()\n",
        "        else:\n",
        "            decoded = self.decoder.compute(eval_res[0])\n",
        "\n",
        "        # map labels (numbers) to character string\n",
        "        texts = self.decoder_output_to_text(decoded, num_batch_elements)\n",
        "\n",
        "        # feed RNN output and recognized text into CTC loss to compute labeling probability\n",
        "        probs = None\n",
        "        if calc_probability:\n",
        "            sparse = self.to_sparse(batch.gt_texts) if probability_of_gt else self.to_sparse(texts)\n",
        "            ctc_input = eval_res[1]\n",
        "            eval_list = self.loss_per_element\n",
        "            feed_dict = {self.saved_ctc_input: ctc_input, self.gt_texts: sparse,\n",
        "                         self.seq_len: [max_text_len] * num_batch_elements, self.is_train: False}\n",
        "            loss_vals = self.sess.run(eval_list, feed_dict)\n",
        "            probs = np.exp(-loss_vals)\n",
        "\n",
        "        # dump the output of the NN to CSV file(s)\n",
        "        if self.dump:\n",
        "            self.dump_nn_output(eval_res[1])\n",
        "\n",
        "        return texts, probs\n",
        "\n",
        "    def save(self) -> None:\n",
        "        \"\"\"Save model to file.\"\"\"\n",
        "        self.snap_ID += 1\n",
        "        self.saver.save(self.sess, '../model/snapshot', global_step=self.snap_ID)\n"
      ],
      "metadata": {
        "id": "jUtcC68hX-en"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from typing import Tuple\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "from dataloader_iam import Batch\n",
        "\n",
        "\n",
        "class Preprocessor:\n",
        "    def __init__(self,\n",
        "                 img_size: Tuple[int, int],\n",
        "                 padding: int = 0,\n",
        "                 dynamic_width: bool = False,\n",
        "                 data_augmentation: bool = False,\n",
        "                 line_mode: bool = False) -> None:\n",
        "        # dynamic width only supported when no data augmentation happens\n",
        "        assert not (dynamic_width and data_augmentation)\n",
        "        # when padding is on, we need dynamic width enabled\n",
        "        assert not (padding > 0 and not dynamic_width)\n",
        "\n",
        "        self.img_size = img_size\n",
        "        self.padding = padding\n",
        "        self.dynamic_width = dynamic_width\n",
        "        self.data_augmentation = data_augmentation\n",
        "        self.line_mode = line_mode\n",
        "\n",
        "    @staticmethod\n",
        "    def _truncate_label(text: str, max_text_len: int) -> str:\n",
        "        \"\"\"\n",
        "        Function ctc_loss can't compute loss if it cannot find a mapping between text label and input\n",
        "        labels. Repeat letters cost double because of the blank symbol needing to be inserted.\n",
        "        If a too-long label is provided, ctc_loss returns an infinite gradient.\n",
        "        \"\"\"\n",
        "        cost = 0\n",
        "        for i in range(len(text)):\n",
        "            if i != 0 and text[i] == text[i - 1]:\n",
        "                cost += 2\n",
        "            else:\n",
        "                cost += 1\n",
        "            if cost > max_text_len:\n",
        "                return text[:i]\n",
        "        return text\n",
        "\n",
        "    def _simulate_text_line(self, batch: Batch) -> Batch:\n",
        "        \"\"\"Create image of a text line by pasting multiple word images into an image.\"\"\"\n",
        "\n",
        "        default_word_sep = 30\n",
        "        default_num_words = 5\n",
        "\n",
        "        # go over all batch elements\n",
        "        res_imgs = []\n",
        "        res_gt_texts = []\n",
        "        for i in range(batch.batch_size):\n",
        "            # number of words to put into current line\n",
        "            num_words = random.randint(1, 8) if self.data_augmentation else default_num_words\n",
        "\n",
        "            # concat ground truth texts\n",
        "            curr_gt = ' '.join([batch.gt_texts[(i + j) % batch.batch_size] for j in range(num_words)])\n",
        "            res_gt_texts.append(curr_gt)\n",
        "\n",
        "            # put selected word images into list, compute target image size\n",
        "            sel_imgs = []\n",
        "            word_seps = [0]\n",
        "            h = 0\n",
        "            w = 0\n",
        "            for j in range(num_words):\n",
        "                curr_sel_img = batch.imgs[(i + j) % batch.batch_size]\n",
        "                curr_word_sep = random.randint(20, 50) if self.data_augmentation else default_word_sep\n",
        "                h = max(h, curr_sel_img.shape[0])\n",
        "                w += curr_sel_img.shape[1]\n",
        "                sel_imgs.append(curr_sel_img)\n",
        "                if j + 1 < num_words:\n",
        "                    w += curr_word_sep\n",
        "                    word_seps.append(curr_word_sep)\n",
        "\n",
        "            # put all selected word images into target image\n",
        "            target = np.ones([h, w], np.uint8) * 255\n",
        "            x = 0\n",
        "            for curr_sel_img, curr_word_sep in zip(sel_imgs, word_seps):\n",
        "                x += curr_word_sep\n",
        "                y = (h - curr_sel_img.shape[0]) // 2\n",
        "                target[y:y + curr_sel_img.shape[0]:, x:x + curr_sel_img.shape[1]] = curr_sel_img\n",
        "                x += curr_sel_img.shape[1]\n",
        "\n",
        "            # put image of line into result\n",
        "            res_imgs.append(target)\n",
        "\n",
        "        return Batch(res_imgs, res_gt_texts, batch.batch_size)\n",
        "\n",
        "    def process_img(self, img: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Resize to target size, apply data augmentation.\"\"\"\n",
        "\n",
        "        # there are damaged files in IAM dataset - just use black image instead\n",
        "        if img is None:\n",
        "            img = np.zeros(self.img_size[::-1])\n",
        "\n",
        "        # data augmentation\n",
        "        img = img.astype(np.float)\n",
        "        if self.data_augmentation:\n",
        "            # photometric data augmentation\n",
        "            if random.random() < 0.25:\n",
        "                def rand_odd():\n",
        "                    return random.randint(1, 3) * 2 + 1\n",
        "                img = cv2.GaussianBlur(img, (rand_odd(), rand_odd()), 0)\n",
        "            if random.random() < 0.25:\n",
        "                img = cv2.dilate(img, np.ones((3, 3)))\n",
        "            if random.random() < 0.25:\n",
        "                img = cv2.erode(img, np.ones((3, 3)))\n",
        "\n",
        "            # geometric data augmentation\n",
        "            wt, ht = self.img_size\n",
        "            h, w = img.shape\n",
        "            f = min(wt / w, ht / h)\n",
        "            fx = f * np.random.uniform(0.75, 1.05)\n",
        "            fy = f * np.random.uniform(0.75, 1.05)\n",
        "\n",
        "            # random position around center\n",
        "            txc = (wt - w * fx) / 2\n",
        "            tyc = (ht - h * fy) / 2\n",
        "            freedom_x = max((wt - fx * w) / 2, 0)\n",
        "            freedom_y = max((ht - fy * h) / 2, 0)\n",
        "            tx = txc + np.random.uniform(-freedom_x, freedom_x)\n",
        "            ty = tyc + np.random.uniform(-freedom_y, freedom_y)\n",
        "\n",
        "            # map image into target image\n",
        "            M = np.float32([[fx, 0, tx], [0, fy, ty]])\n",
        "            target = np.ones(self.img_size[::-1]) * 255\n",
        "            img = cv2.warpAffine(img, M, dsize=self.img_size, dst=target, borderMode=cv2.BORDER_TRANSPARENT)\n",
        "\n",
        "            # photometric data augmentation\n",
        "            if random.random() < 0.5:\n",
        "                img = img * (0.25 + random.random() * 0.75)\n",
        "            if random.random() < 0.25:\n",
        "                img = np.clip(img + (np.random.random(img.shape) - 0.5) * random.randint(1, 25), 0, 255)\n",
        "            if random.random() < 0.1:\n",
        "                img = 255 - img\n",
        "\n",
        "        # no data augmentation\n",
        "        else:\n",
        "            if self.dynamic_width:\n",
        "                ht = self.img_size[1]\n",
        "                h, w = img.shape\n",
        "                f = ht / h\n",
        "                wt = int(f * w + self.padding)\n",
        "                wt = wt + (4 - wt) % 4\n",
        "                tx = (wt - w * f) / 2\n",
        "                ty = 0\n",
        "            else:\n",
        "                wt, ht = self.img_size\n",
        "                h, w = img.shape\n",
        "                f = min(wt / w, ht / h)\n",
        "                tx = (wt - w * f) / 2\n",
        "                ty = (ht - h * f) / 2\n",
        "\n",
        "            # map image into target image\n",
        "            M = np.float32([[f, 0, tx], [0, f, ty]])\n",
        "            target = np.ones([ht, wt]) * 255\n",
        "            img = cv2.warpAffine(img, M, dsize=(wt, ht), dst=target, borderMode=cv2.BORDER_TRANSPARENT)\n",
        "\n",
        "        # transpose for TF\n",
        "        img = cv2.transpose(img)\n",
        "\n",
        "        # convert to range [-1, 1]\n",
        "        img = img / 255 - 0.5\n",
        "        return img\n",
        "\n",
        "    def process_batch(self, batch: Batch) -> Batch:\n",
        "        if self.line_mode:\n",
        "            batch = self._simulate_text_line(batch)\n",
        "\n",
        "        res_imgs = [self.process_img(img) for img in batch.imgs]\n",
        "        max_text_len = res_imgs[0].shape[0] // 4\n",
        "        res_gt_texts = [self._truncate_label(gt_text, max_text_len) for gt_text in batch.gt_texts]\n",
        "        return Batch(res_imgs, res_gt_texts, batch.batch_size)\n",
        "\n",
        "\n",
        "def main():\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    img = cv2.imread('../data/test.png', cv2.IMREAD_GRAYSCALE)\n",
        "    img_aug = Preprocessor((256, 32), data_augmentation=True).process_img(img)\n",
        "    plt.subplot(121)\n",
        "    plt.imshow(img, cmap='gray')\n",
        "    plt.subplot(122)\n",
        "    plt.imshow(cv2.transpose(img_aug) + 0.5, cmap='gray', vmin=0, vmax=1)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "p190wGepYA5N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}